
# Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods

## Overview

A highâ€fidelity Unityâ€‘based framework implementing rewardâ€‘augmented learning strategies for continuousâ€‘control autonomous parking (AP). RARLAP enables rapid, safe, and adaptable policy optimization in tight spatial environments by structuring reward functions to guide exploration and convergence.

---
## Key Features

- ğŸ® **Custom 3D Simulation**  
  - ğŸš— Built on Unity with realistic vehicle kinematics, sensor noise, and dynamic obstacles.  
  - ğŸ…¿ï¸ Supports configurable parking scenarios: parallel, perpendicular, and angled stalls.

- ğŸ¯ **Structured Reward Strategies**  
  - ğŸ¥… **Goalâ€‘Only Reward (GOR):** Sparse reward upon successful parking.  
  - ğŸ“ **Dense Proximity Reward (DPR):** Continuous shaping based on distance to the target pose and obstacle penalties.  
  - ğŸªœ **Milestoneâ€‘Augmented Reward (MAR):** Discrete subâ€‘goals (e.g. alignment, entry waypoints) to scaffold learning.

- ğŸ”„ **Onâ€‘Policy & Offâ€‘Policy Algorithms**  
  - âš™ï¸ Compatible with both onâ€‘policy methods (e.g. PPO, TRPO) and offâ€‘policy methods (e.g. DDPG, SAC).  
  - ğŸ”§ Modular trainer scripts allow swapping algorithms and tuning hyperparameters.

- ğŸ§ª **Reproducible Experiments**  
  - ğŸ“Š Comprehensive logging via TensorBoard and CSV outputs.  
  - ğŸ“ Predefined configs for all reward strategies and learning algorithms.  
  - ğŸ³ Dockerfile and conda environment for seamless setup.

- ğŸ“ˆ **Empirical Results**  
  - ğŸ‰ Onâ€‘policy MAR achieves **91%** average success rate, smooth trajectories, and robust obstacle avoidance.  
  - ğŸ“‰ Benchmark comparisons highlight RARLAPâ€™s superior convergence speed and safety margins over baseline controllers.

## Features

- ğŸ§  **Reward-Augmented RL**: Supports sparse, dense, and milestone-based reward shaping strategies to guide training behavior in complex parking scenarios.
- ğŸ® **Unity 3D Simulation**: High-fidelity simulation environment with realistic physics, Ackermann steering, and customizable parking configurations.
- ğŸ”„ **Continuous Control**: Designed for continuous action spaces, suitable for real-world precision steering tasks.
- âš™ï¸ **ML-Agents Integration**: Fully compatible with Unity ML-Agents Toolkit for seamless agent training and evaluation.
- ğŸ“‚ **Modular & Extensible**: Codebase structured for easy extension, benchmarking, and integration with new algorithms or tasks.

---
![Car_Parking_Project](Car_Parking_Project.jpg)
![Inference](PPO-MBR_inference.gif)


## Project Strcture
- Project Files: contain the Prefeb for Unity Simulation
- Related scripts are provided in the scripts folder
- Pretrained weights for SAC and PPO along with training configuations are given with filenames SAC_MBR and PPO_MBR respectively.

## Cite 
@misc{suleman2025rewardaugmentedreinforcementlearningcontinuous,
      title={Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods}, 
      author={Ahmad Suleman and Misha Urooj Khan and Zeeshan Kaleem and Ali H. Alenezi and Iqra Shabbir Sinem Coleri and Chau Yuen},
      year={2025},
      eprint={2507.19642},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2507.19642}, 
}
  
